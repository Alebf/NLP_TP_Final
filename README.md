![Image text](https://github.com/Alebf/NLP_TP_Final/blob/master/logo.png)

# NLP_TP_Final
Repositorio del trabajo práctico final de la materia **Procesamiento del lenguaje natural (NLP)**.

La materia propone la resolución de un desafío por clase.

**Clase 1**:
Este desafío tiene como objetivo utilizar las técnicas básicas para el procesamiento de lenguaje natural.

Para su resolución se utilizó **OneHotEncoding, frequency vectors y TF-IDF**. Se aplicaron estas técnicas sobre un corpus y los vectores resultantes fueron comparados
mediante una medida de similitud denominada **similitud de coseno**.

[Desafío nro 1](https://github.com/Alebf/NLP_TP_Final/blob/master/Desaf%C3%ADo%20nro%201/Desaf%C3%ADo%20nro%201.ipynb)

**Clase 2**:
El desafío de esta clase propone desarrollar un bot simple capaz de identificar el contexto del texto que se le ingresa y redirigirlo a un conjunto de respuestas
preestablecidas.

Se utilizó una librería de NLP de Stanford **SpaCy-Stanza** con el diccionario en español debido a que se trabajó con artículos descargados de wikipedia sobre fútbol en español.

[Desafío nro 2](https://github.com/Alebf/NLP_TP_Final/blob/master/Desaf%C3%ADo%20nro%202/Desaf%C3%ADo_nro_2.ipynb)

**Clase 3**:
En esta clase se propone aprender a generar y utilizar los **Word Embedding** mediante el uso de la librería **Gensim**.

Para este trabajo se descargó el último discurso presidencial de apertura de sesiones ordinarias en el congreso en español y se intentó identificar las palabras mas cercanas y lejanas utilizando la similitud de coseno.

[Desafío nro 3](https://github.com/Alebf/NLP_TP_Final/blob/master/Desaf%C3%ADo%20nro%203/Desaf%C3%ADo_nro_3.ipynb)

**Clase 4**:
Se propone tratar con problemas de secuencias y estimación de próxima palabra utilizando **Embedding de keras y LSTM**

Se armó un dataset con todos los discursos presidenciales del año 2022.

[Desafío nro 4](https://github.com/Alebf/NLP_TP_Final/blob/master/Desaf%C3%ADo%20nro%204/Desaf%C3%ADo_nro_4.ipynb)

**Clase 5**:
En este desafío se profundiza la utilización de las redes **LSTM**, entrenando nuestros propios embedding y utilizando embedding pre-entrenados, intentando determinar la intención detrás de un texto dado.

Se trabajó con un dataset desbalanceado y se demostró que independientemente de si se entrenan los embedding o se utilizan embedding pre-entrenados no se obtiene una buena performance en dataset desbalanceado.

[Desafío nro 5](https://github.com/Alebf/NLP_TP_Final/blob/master/Desaf%C3%ADo%20nro%205/Desaf%C3%ADo_nro_5.ipynb)

**Clase 6**:
El último desafío propone la utilización de una arquitectura **Seq2Seq**.

Para esto se propuso hacer un bot conversacional (preguntas y respuestas) 

[Desafío nro 6](https://github.com/Alebf/NLP_TP_Final/blob/master/Desaf%C3%ADo%20nro%206/Desaf%C3%ADo_nro_6.ipynb)



